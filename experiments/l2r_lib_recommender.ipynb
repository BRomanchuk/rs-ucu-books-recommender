{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bromanchuk/.venv39/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from libreco.data import DatasetPure\n",
    "from libreco.algorithms import BPR, WideDeep\n",
    "from libreco.evaluation import evaluate\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/4m_t_82936ggv17n0g0t7_dr0000gn/T/ipykernel_14827/4047143737.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  users_df = pd.read_csv('../data/Users.csv', delimiter=';')\n"
     ]
    }
   ],
   "source": [
    "books_df = pd.read_csv('../data/Books.csv', delimiter=';', low_memory=False)\n",
    "ratings_df = pd.read_csv('../data/Ratings.csv', delimiter=';')\n",
    "users_df = pd.read_csv('../data/Users.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.columns = ['user_id', 'item_id', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = ratings_df[ratings_df['label'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_str_to_int = {book_id: i for i, book_id in enumerate(ratings_df['item_id'].unique())}\n",
    "user_str_to_int = {user_id: i for i, user_id in enumerate(ratings_df['user_id'].unique())}\n",
    "\n",
    "ratings_df['item'] = ratings_df['item_id'].map(book_str_to_int).to_numpy()\n",
    "ratings_df['user'] = ratings_df['user_id'].map(user_str_to_int).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Eval Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df = train_test_split(ratings_df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_user_mask = eval_df['user'].isin(train_df['user'].unique())\n",
    "eval_item_mask = eval_df['item'].isin(train_df['item'].unique())\n",
    "\n",
    "eval_df = eval_df[eval_user_mask & eval_item_mask]\n",
    "\n",
    "# 'user', 'item' must be the first two columns of the dataframe\n",
    "train_df = train_df[['user', 'item', 'label']]\n",
    "eval_df = eval_df[['user', 'item', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Prepare the dataset for LibRecommender\n",
    "train_data, data_info = DatasetPure.build_trainset(train_df)\n",
    "eval_data = DatasetPure.build_evalset(eval_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Personalized Ranking and Wide Deep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 16\n",
    "n_epochs_bpr = 1\n",
    "n_epochs_wide = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 256\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Initialize the BPR model\n",
    "bpr = BPR(\n",
    "    task=\"ranking\",  # This specifies that we're performing a ranking task\n",
    "    data_info=data_info,\n",
    "    embed_size=embedding_size,  # Size of the embedding vectors\n",
    "    n_epochs=n_epochs_bpr,  # Number of training epochs\n",
    "    lr=learning_rate,  # Learning rate\n",
    "    reg=None,  # Regularization parameter, can be tuned\n",
    "    batch_size=batch_size,  # Batch size for training\n",
    "    num_neg=1,  # Number of negative samples per positive sample\n",
    "    use_tf=True,  # Whether to use TensorFlow backend\n",
    "    seed=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the WideDeep model\n",
    "wide_deep = WideDeep(\n",
    "    task=\"ranking\",  # Use \"rating\" for explicit feedback datasets\n",
    "    data_info=data_info,\n",
    "    embed_size=embedding_size,\n",
    "    n_epochs=n_epochs_wide,\n",
    "    lr={'wide': learning_rate, 'deep': learning_rate/10},\n",
    "    batch_size=batch_size,\n",
    "    use_bn=True,  # Batch normalization\n",
    "    hidden_units=[64, 32],  # Neural network architecture\n",
    "    reg=None,  # Regularization; you can specify l2 or l1 here\n",
    "    dropout_rate=None,  # Optionally add dropout to prevent overfitting\n",
    "    num_neg=1,  # Number of negative samples per positive sample\n",
    "    seed=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bpr.fit(\n",
    "    train_data, \n",
    "    neg_sampling=True,\n",
    "    verbose=2, \n",
    "    shuffle=True, \n",
    "    eval_data=eval_data, \n",
    "    metrics=[\"ndcg\", \"precision\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001b[35m2024-08-10 16:19:32\u001b[0m\n",
      "WARNING:tensorflow:From /Users/bromanchuk/.venv39/lib/python3.9/site-packages/keras/layers/normalization/batch_normalization.py:562: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bromanchuk/.venv39/lib/python3.9/site-packages/libreco/layers/dense.py:31: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  net = tf.layers.batch_normalization(net, training=is_training)\n",
      "2024-08-10 16:19:32,772 - WARNING - From /Users/bromanchuk/.venv39/lib/python3.9/site-packages/keras/layers/normalization/batch_normalization.py:562: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \u001b[33m3,868,095\u001b[0m | embedding params: \u001b[33m3,863,773\u001b[0m | network params: \u001b[33m4,322\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bromanchuk/.venv39/lib/python3.9/site-packages/libreco/layers/dense.py:39: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  net = tf.layers.batch_normalization(net, training=is_training)\n",
      "2024-08-10 16:19:33.099230: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2024-08-10 16:19:33.110035: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "train: 100%|██████████| 2711/2711 [00:22<00:00, 120.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 22.516s\n",
      "\t \u001b[32mtrain_loss: 0.6345\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise: 100%|██████████| 16331/16331 [11:41<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval ndcg@10: 0.0095\n",
      "\t eval precision@10: 0.0023\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 2711/2711 [00:23<00:00, 117.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 elapsed: 23.152s\n",
      "\t \u001b[32mtrain_loss: 0.5785\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise: 100%|██████████| 16331/16331 [13:50<00:00, 19.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval ndcg@10: 0.0086\n",
      "\t eval precision@10: 0.0019\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 2711/2711 [00:34<00:00, 77.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 elapsed: 34.834s\n",
      "\t \u001b[32mtrain_loss: 0.4602\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise: 100%|██████████| 16331/16331 [17:50<00:00, 15.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval ndcg@10: 0.0084\n",
      "\t eval precision@10: 0.0018\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 2711/2711 [00:25<00:00, 107.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 elapsed: 25.178s\n",
      "\t \u001b[32mtrain_loss: 0.3211\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise:  64%|██████▍   | 10497/16331 [12:24<06:19, 15.36it/s] "
     ]
    }
   ],
   "source": [
    "wide_deep.fit(\n",
    "    train_data, \n",
    "    neg_sampling=True,\n",
    "    verbose=2, \n",
    "    shuffle=True, \n",
    "    eval_data=eval_data, \n",
    "    metrics=[\"ndcg\", \"precision\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the model\n",
    "bpr_eval_result = evaluate(bpr, eval_data, neg_sampling=True, metrics=[\"ndcg\", \"precision\", \"recall\"])\n",
    "print(f\"Evaluation Results (BPR):\\n{bpr_eval_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the model\n",
    "wide_deep_eval_result = evaluate(wide_deep, eval_data, neg_sampling=True, metrics=[\"ndcg\", \"precision\", \"recall\"])\n",
    "print(f\"Evaluation Results (WideDeep):\\n{wide_deep_eval_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = train_df.iloc[:10]['user'].values\n",
    "item_id = train_df.iloc[:10]['item'].values\n",
    "prediction = bpr.predict(user_id, item_id)\n",
    "print(f\"BPR Prediction for user {user_id} and item {item_id}: {prediction}\")\n",
    "prediction = wide_deep.predict(user_id, item_id)\n",
    "print(f\"WideDeep Prediction for user {user_id} and item {item_id}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
