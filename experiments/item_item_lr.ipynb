{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item to item collaborative filtering\n",
    "\n",
    "An idea of item-to-item collaborative filtering approach is that recommendations are based on finding similarities between the items in terms of how they are rated by the users.\\\n",
    "So, the matrix of interactions is built, and cosine similarities between the items are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\\\n",
    "For this task we need the ratings table only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276725</td>\n",
       "      <td>034545104X</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276727</td>\n",
       "      <td>0446520802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user        item  label\n",
       "0  276725  034545104X      0\n",
       "1  276726  0155061224      5\n",
       "2  276727  0446520802      0\n",
       "3  276729  052165615X      3\n",
       "4  276729  0521795028      6"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df = pd.read_csv('../data/Ratings.csv', delimiter=';', dtype={'User-ID': np.int32, 'ISBN': str, 'Rating': np.int8})\n",
    "ratings_df.columns = ['user', 'item', 'label']\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we drop the rows with:\n",
    "- missing data (all the values are crucial)\n",
    "- zero ratings (as they have no impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ratings: 1149780\n",
      "Total ratings (cleaned): 433671\n"
     ]
    }
   ],
   "source": [
    "print('Total ratings:', ratings_df.shape[0])\n",
    "\n",
    "ratings_df = ratings_df[ratings_df['label'] > 0]\n",
    "ratings_df.dropna()\n",
    "\n",
    "print('Total ratings (cleaned):', ratings_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After experimenting with straightfoward approach with sparse matrices (ItemRecommender class) we swithced to LibRecommender, which is far superior in terms of performance due to a number of optimizations.\\\n",
    "The tasks performed below are:\n",
    "- train/eval split\n",
    "- data preparation to LibReco model input\n",
    "- model creation with 10 nearest neighbors configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001b[35m2024-08-11 22:08:04\u001b[0m\n",
      "Final block size and num: (1251, 127)\n",
      "sim_matrix elapsed: 113.577s\n",
      "sim_matrix, shape: (158852, 158852), num_elements: 96093388, density: 0.3808 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "top_k: 100%|██████████| 158852/158852 [00:37<00:00, 4252.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from libreco.algorithms import ItemCF\n",
    "from libreco.data import DatasetPure\n",
    "from libreco.data import random_split\n",
    "\n",
    "ratings_df = ratings_df[[\"user\", \"item\", \"label\"]]\n",
    "train_df, eval_df = random_split(ratings_df, test_size=0.2)\n",
    "\n",
    "train_data, data_info = DatasetPure.build_trainset(train_df)\n",
    "eval_data = DatasetPure.build_evalset(eval_df)\n",
    "\n",
    "model = ItemCF(task=\"ranking\", \n",
    "               data_info=data_info,\n",
    "               k_sim=10, \n",
    "               sim_type=\"cosine\", \n",
    "               min_common=1)\n",
    "\n",
    "model.fit(train_data, neg_sampling=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model on 3 metrics: precision, recall and NDCG (Normalized Discounted Cumulative Gain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise:   1%|          | 119/16407 [00:01<02:31, 107.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mno suitable recommendation for user 39925, return default recommendation\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise:   1%|▏         | 221/16407 [00:01<01:20, 202.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mno suitable recommendation for user 25412, return default recommendation\u001b[0m\n",
      "\u001b[31mno suitable recommendation for user 44239, return default recommendation\u001b[0m\n",
      "\u001b[31mno suitable recommendation for user 12084, return default recommendation\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise:   2%|▏         | 287/16407 [00:01<01:03, 254.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mno suitable recommendation for user 56690, return default recommendation\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise:   3%|▎         | 480/16407 [00:02<00:47, 338.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mno suitable recommendation for user 17623, return default recommendation\u001b[0m\n",
      "\u001b[31mno suitable recommendation for user 53915, return default recommendation\u001b[0m\n",
      "\u001b[31mno suitable recommendation for user 66434, return default recommendation\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise:   4%|▍         | 734/16407 [00:03<00:32, 478.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mno suitable recommendation for user 13863, return default recommendation\u001b[0m\n",
      "\u001b[31mno suitable recommendation for user 1751, return default recommendation\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise: 100%|██████████| 16407/16407 [00:11<00:00, 1394.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "{'precision': 0.004382275857865545, 'recall': 0.014942970740036218, 'ndcg': 0.033249598472747145}\n"
     ]
    }
   ],
   "source": [
    "from libreco.evaluation import evaluate\n",
    "\n",
    "eval_result = evaluate(model, eval_data, neg_sampling=True, metrics=[ \"precision\", \"recall\", \"ndcg\"])\n",
    "print(f\"Evaluation Results:\\n{eval_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got pretty average results. Moreover, there were items with no recommendations due to not enough ratings. So, then we decided to remove rare books from the dataset for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing rare books**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ratings (rare books excluded): 94503\n"
     ]
    }
   ],
   "source": [
    "rating_count=pd.DataFrame(ratings_df[\"item\"].value_counts())\n",
    "rare_books=rating_count[rating_count[\"count\"]<=20].index\n",
    "ratings_df=ratings_df[~ratings_df[\"item\"].isin(rare_books)]\n",
    "\n",
    "print('Total ratings (rare books excluded):', ratings_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001b[35m2024-08-11 22:10:48\u001b[0m\n",
      "Final block size and num: (2034, 1)\n",
      "sim_matrix elapsed: 0.080s\n",
      "sim_matrix, shape: (2034, 2034), num_elements: 1217048, density: 29.4175 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "top_k: 100%|██████████| 2034/2034 [00:00<00:00, 5034.83it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df, eval_df = random_split(ratings_df, test_size=0.2)\n",
    "\n",
    "train_data, data_info = DatasetPure.build_trainset(train_df)\n",
    "eval_data = DatasetPure.build_evalset(eval_df)\n",
    "\n",
    "# Step 3: Build and train the model\n",
    "model = ItemCF(task=\"ranking\", \n",
    "               data_info=data_info,\n",
    "               k_sim=10, \n",
    "               sim_type=\"cosine\", \n",
    "               min_common=1)\n",
    "\n",
    "model.fit(train_data, neg_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise: 100%|██████████| 1676/1676 [00:02<00:00, 704.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "{'precision': 0.02199015366253916, 'recall': 0.10206671918612688, 'ndcg': 0.11273428439190147}\n"
     ]
    }
   ],
   "source": [
    "from libreco.evaluation import evaluate\n",
    "\n",
    "eval_result = evaluate(model, eval_data, neg_sampling=True, metrics=[ \"precision\", \"recall\", \"ndcg\"])\n",
    "print(f\"Evaluation Results:\\n{eval_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the precision is not big. But 5-times better compared to a dataset with rare books."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys_lr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
