{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import data_preprocessing as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/4m_t_82936ggv17n0g0t7_dr0000gn/T/ipykernel_78370/4047143737.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  users_df = pd.read_csv('../data/Users.csv', delimiter=';')\n"
     ]
    }
   ],
   "source": [
    "books_df = pd.read_csv('../data/Books.csv', delimiter=';', low_memory=False)\n",
    "ratings_df = pd.read_csv('../data/Ratings.csv', delimiter=';')\n",
    "users_df = pd.read_csv('../data/Users.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_str = ratings_df['User-ID']\n",
    "book_ids_str = ratings_df['ISBN']\n",
    "ratings = ratings_df['Rating'].to_numpy()\n",
    "\n",
    "book_str_to_int = {book_id: i for i, book_id in enumerate(book_ids_str.unique())}\n",
    "user_str_to_int = {user_id: i for i, user_id in enumerate(user_ids_str.unique())}\n",
    "\n",
    "book_ids = book_ids_str.map(book_str_to_int).to_numpy()\n",
    "user_ids = user_ids_str.map(user_str_to_int).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in book_ids:\n",
    "    if type(u) != np.int64:\n",
    "        print(u, type(u))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ...,  12065,  78598, 340555])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BookCrossingDataset(Dataset):\n",
    "    def __init__(self, user_ids, book_ids, ratings):\n",
    "        self.user_ids = user_ids\n",
    "        self.book_ids = book_ids\n",
    "        self.ratings = ratings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.book_ids[idx], self.ratings[idx]\n",
    "\n",
    "class LTRModel(nn.Module):\n",
    "    def __init__(self, num_users, num_books, embedding_dim):\n",
    "        super(LTRModel, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.book_embedding = nn.Embedding(num_books, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, user_id, book_id):\n",
    "        user_emb = self.user_embedding(user_id)\n",
    "        book_emb = self.book_embedding(book_id)\n",
    "        x = torch.cat([user_emb, book_emb], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        score = 5*F.sigmoid(self.fc3(x))\n",
    "        return score\n",
    "\n",
    "def pairwise_hinge_loss(pos_scores, neg_scores, margin=1.0):\n",
    "    loss = torch.mean(torch.clamp(margin - pos_scores + neg_scores, min=0))\n",
    "    return loss\n",
    "\n",
    "# Example of training the model\n",
    "def train_model(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for user_id, book_id, rating in tqdm(train_loader):\n",
    "        user_id = user_id.to(device)\n",
    "        book_id = book_id.to(device)\n",
    "        rating = rating.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pos_scores = model(user_id, book_id)\n",
    "        neg_scores = model(user_id, torch.randint(0, num_books, book_id.size(), device=device))\n",
    "\n",
    "        loss = pairwise_hinge_loss(pos_scores, neg_scores)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Hyperparameters and dataset setup\n",
    "embedding_dim = 16\n",
    "num_users = len(np.unique(user_ids))  # Replace with actual number of users\n",
    "num_books = len(np.unique(book_ids))  # Replace with actual number of books\n",
    "batch_size = 64\n",
    "learning_rate = 0.003\n",
    "\n",
    "\n",
    "# Replace these with actual data\n",
    "# user_ids = torch.randint(0, num_users, (100000,))\n",
    "# book_ids = torch.randint(0, num_books, (100000,))\n",
    "# ratings = torch.randint(0, 2, (100000,))\n",
    "\n",
    "dataset = BookCrossingDataset(user_ids, book_ids, ratings)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = LTRModel(num_users, num_books, embedding_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17966/17966 [05:51<00:00, 51.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.3609\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17966/17966 [06:13<00:00, 48.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.3501\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17966/17966 [06:02<00:00, 49.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.3399\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17966/17966 [06:05<00:00, 49.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.3303\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17966/17966 [05:50<00:00, 51.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.3208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    train_model(model, train_loader, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  5,  0, ..., 10, 10,  8])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8237],\n",
       "        [ 0.5236],\n",
       "        [ 2.4722],\n",
       "        [-0.0794],\n",
       "        [-1.2224],\n",
       "        [-0.0794],\n",
       "        [ 1.1973],\n",
       "        [ 0.9286],\n",
       "        [ 2.1904],\n",
       "        [-1.4322],\n",
       "        [ 2.0835],\n",
       "        [ 2.3873],\n",
       "        [ 2.2468],\n",
       "        [ 2.8970],\n",
       "        [ 1.4793],\n",
       "        [ 1.0199],\n",
       "        [ 2.3285],\n",
       "        [ 2.1610],\n",
       "        [ 1.4643],\n",
       "        [ 2.5709],\n",
       "        [ 1.1719],\n",
       "        [ 0.8368],\n",
       "        [-0.0794],\n",
       "        [-0.0794],\n",
       "        [ 0.5975],\n",
       "        [ 2.4500],\n",
       "        [-0.0794],\n",
       "        [ 1.4538],\n",
       "        [ 1.9972],\n",
       "        [ 2.2352],\n",
       "        [-0.9513],\n",
       "        [ 1.5490],\n",
       "        [-3.6300],\n",
       "        [ 1.7268],\n",
       "        [ 2.7080],\n",
       "        [-0.6589],\n",
       "        [-2.2945],\n",
       "        [-1.1603],\n",
       "        [-2.0338],\n",
       "        [ 1.4952],\n",
       "        [-1.1852],\n",
       "        [-1.7919],\n",
       "        [-2.9719],\n",
       "        [-0.0794],\n",
       "        [-0.0794],\n",
       "        [-3.5794],\n",
       "        [-1.3562],\n",
       "        [-2.5260],\n",
       "        [-2.0439],\n",
       "        [-2.7487],\n",
       "        [-0.6022],\n",
       "        [-2.6267],\n",
       "        [-2.0895],\n",
       "        [-4.3284],\n",
       "        [ 0.4846],\n",
       "        [-2.5927],\n",
       "        [-0.4679],\n",
       "        [-0.0794],\n",
       "        [ 0.3266],\n",
       "        [ 1.7440],\n",
       "        [ 0.7658],\n",
       "        [ 0.6286],\n",
       "        [ 0.1118],\n",
       "        [-0.1225]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor(np.arange(64)).to(torch.int).to(device), torch.Tensor(np.arange(64)).to(torch.int).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 340553, 340554, 340555])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(book_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43muser_id\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'user_id' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bromanchuk/Desktop/my/vscode_projects/rs-ucu-books-recommender/experiments/../src/utils/data_preprocessing.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_result['Original_NaN'] = df_result['Age'].isna()\n",
      "/Users/bromanchuk/Desktop/my/vscode_projects/rs-ucu-books-recommender/experiments/../src/utils/data_preprocessing.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_result['Age'] = pd.to_numeric(df_result['Age'], errors='coerce')\n",
      "/Users/bromanchuk/Desktop/my/vscode_projects/rs-ucu-books-recommender/experiments/../src/utils/data_preprocessing.py:21: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_result = df_result[~(users['Age'].isna() & ~df_result['Original_NaN'])]\n"
     ]
    }
   ],
   "source": [
    "books, ratings, users,  = prep.preprocess(books, ratings, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Sample data for demonstration purposes\n",
    "# books_data = {\n",
    "#     'ISBN': ['0002005018', '0002005019', '0002005020'],\n",
    "#     'Title': ['Book1', 'Book2', 'Book3'],\n",
    "#     'Author': ['Author1', 'Author2', 'Author3'],\n",
    "#     'Year': [2001, 2002, 2003]\n",
    "# }\n",
    "\n",
    "# users_data = {\n",
    "#     'User-ID': [1, 2, 3],\n",
    "#     'Age': [23, 34, 45]\n",
    "# }\n",
    "\n",
    "# ratings_data = {\n",
    "#     'User-ID': [1, 1, 2, 2, 3, 3],\n",
    "#     'ISBN': ['0002005018', '0002005019', '0002005018', '0002005020', '0002005019', '0002005020'],\n",
    "#     'Rating': [5, 3, 4, 2, 1, 5]\n",
    "# }\n",
    "\n",
    "# books = pd.DataFrame(books_data)\n",
    "# users = pd.DataFrame(users_data)\n",
    "# ratings = pd.DataFrame(ratings_data)\n",
    "# Preprocessing\n",
    "label_encoders = {\n",
    "    'ISBN': LabelEncoder(),\n",
    "    'User-ID': LabelEncoder()\n",
    "}\n",
    "\n",
    "books['ISBN'] = label_encoders['ISBN'].fit_transform(books['ISBN'])\n",
    "ratings['ISBN'] = label_encoders['ISBN'].transform(ratings['ISBN'])\n",
    "ratings['User-ID'] = label_encoders['User-ID'].fit_transform(ratings['User-ID'])\n",
    "users['User-ID'] = label_encoders['User-ID'].transform(users['User-ID'])\n",
    "\n",
    "# Merge data\n",
    "data = ratings.merge(books, on='ISBN').merge(users, on='User-ID')\n",
    "\n",
    "# Feature engineering\n",
    "features = data[['User-ID', 'ISBN', 'Age', 'Year']].values\n",
    "labels = data['Rating'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "class LambdaRankLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LambdaRankLoss, self).__init__()\n",
    "\n",
    "    def forward(self, scores, labels):\n",
    "        loss = 0.0\n",
    "        n = scores.size(0)\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if labels[i] > labels[j]:\n",
    "                    S_ij = 1\n",
    "                elif labels[i] < labels[j]:\n",
    "                    S_ij = -1\n",
    "                else:\n",
    "                    S_ij = 0\n",
    "                score_diff = scores[i] - scores[j]\n",
    "                loss += 0.5 * (1.0 - S_ij) - torch.sigmoid(S_ij * score_diff)\n",
    "        loss = loss / (n * (n - 1))\n",
    "        return loss\n",
    "\n",
    "class Learn2RankModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Learn2RankModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Check if MPS is available and set the device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Model parameters\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 128\n",
    "\n",
    "model = Learn2RankModel(input_dim, hidden_dim).to(device)\n",
    "criterion = LambdaRankLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    features_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    labels_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    scores = model(features_tensor).squeeze()\n",
    "    loss = criterion(scores, labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_features_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    test_labels_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "    test_scores = model(test_features_tensor).squeeze()\n",
    "    test_loss = criterion(test_scores, test_labels_tensor)\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class L2RModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def prepare_data(self, user_features, book_features, labels):\n",
    "        \"\"\"\n",
    "        Prepares the training data for the L2R model.\n",
    "        Args:\n",
    "            user_features (pd.DataFrame): DataFrame containing user features.\n",
    "            book_features (pd.DataFrame): DataFrame containing book features.\n",
    "            labels (pd.Series): Series containing relevance labels for training.\n",
    "        Returns:\n",
    "            pd.DataFrame: Combined DataFrame of user and book features.\n",
    "        \"\"\"\n",
    "        data = pd.concat([user_features, book_features], axis=1)\n",
    "        return data, labels\n",
    "\n",
    "    def train(self, user_features, book_features, labels):\n",
    "        \"\"\"\n",
    "        Trains the L2R model.\n",
    "        Args:\n",
    "            user_features (pd.DataFrame): DataFrame containing user features.\n",
    "            book_features (pd.DataFrame): DataFrame containing book features.\n",
    "            labels (pd.Series): Series containing relevance labels for training.\n",
    "        \"\"\"\n",
    "        data, labels = self.prepare_data(user_features, book_features, labels)\n",
    "        \n",
    "        train_data = lgb.Dataset(data, label=labels)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'lambdarank',\n",
    "            'metric': 'ndcg',\n",
    "            'ndcg_at': [1, 3, 5],\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 31,\n",
    "            'min_data_in_leaf': 20\n",
    "        }\n",
    "        \n",
    "        self.model = lgb.train(params, train_data, num_boost_round=100)\n",
    "    \n",
    "    def predict(self, user_features, book_features):\n",
    "        \"\"\"\n",
    "        Predicts the relevance scores for the given features.\n",
    "        Args:\n",
    "            user_features (pd.DataFrame): DataFrame containing user features.\n",
    "            book_features (pd.DataFrame): DataFrame containing book features.\n",
    "        Returns:\n",
    "            np.ndarray: Predicted relevance scores.\n",
    "        \"\"\"\n",
    "        data, _ = self.prepare_data(user_features, book_features, None)\n",
    "        return self.model.predict(data)\n",
    "    \n",
    "    def rank_books(self, user_features, book_features):\n",
    "        \"\"\"\n",
    "        Ranks the books for a given user based on predicted relevance scores.\n",
    "        Args:\n",
    "            user_features (pd.DataFrame): DataFrame containing user features.\n",
    "            book_features (pd.DataFrame): DataFrame containing book features.\n",
    "        Returns:\n",
    "            pd.Series: Series containing the ranking of books.\n",
    "        \"\"\"\n",
    "        scores = self.predict(user_features, book_features)\n",
    "        rankings = np.argsort(scores)[::-1]\n",
    "        return rankings\n",
    "\n",
    "# Example usage:\n",
    "# Assuming user_features_df and book_features_df are prepared DataFrames\n",
    "# and labels is a Series of relevance labels.\n",
    "\n",
    "# l2r = L2RModel()\n",
    "# l2r.train(user_features_df, book_features_df, labels)\n",
    "# rankings = l2r.rank_books(user_features_df, book_features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloenv",
   "language": "python",
   "name": "yoloenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
